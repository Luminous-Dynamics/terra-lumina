# robots.txt for Terra Lumina

User-agent: *
Allow: /

# Disallow private/sensitive pages
Disallow: /data-room
Disallow: /api/
Disallow: /*.json$

# Sitemap location
Sitemap: https://terra.luminousdynamics.org/sitemap.xml

# Crawl rate preferences
# Request: 1 page per 10 seconds to avoid server load
Crawl-delay: 10

# Specific bot preferences
User-agent: Googlebot
Allow: /
Crawl-delay: 5

User-agent: Bingbot
Allow: /
Crawl-delay: 5

User-agent: Slurp
Allow: /
Crawl-delay: 10

# Block common bad bots
User-agent: AhrefsBot
Crawl-delay: 30

User-agent: SemrushBot
Crawl-delay: 30

# Block AI training scrapers (optional - consider your stance)
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

# Social media preview bots (allow for sharing)
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /
